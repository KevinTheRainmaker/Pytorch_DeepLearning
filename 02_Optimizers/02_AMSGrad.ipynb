{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMSGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: <a href='https://arxiv.org/abs/1904.09237'>On the Convergence of Adam and Beyond</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AMSGrad는 Adam 옵티마이저의 변형으로, Adam이 일부 경우에 수렴 문제를 겪는 것을 해결하기 위해 개발되었다. AMSGrad는 이전 Gradient의 최댓값을 이용하여 보다 안정적인 학습률을 제공함으로써, 학습 과정의 안정성을 높이고 더 나은 수렴 성능을 달성하는 것을 목표로 한다.\n",
    "\n",
    "여기선 이전에 서술했던 Adam 옵티마이저를 상속 받아 사용하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch._tensor import Tensor\n",
    "\n",
    "from optimizers import WeightDecay, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMSGrad(Adam):\n",
    "    def __init__(self, params, lr: float = 1e-3, \n",
    "                 betas: Tuple[float, float] = (0.9, 0.999), \n",
    "                 eps: float = 1e-16, weight_decay: WeightDecay = WeightDecay(), \n",
    "                 optimized_update: bool = True, amsgrad: bool = True,\n",
    "                 defaults: Optional[Dict[str, Any]] = None):\n",
    "        '''\n",
    "        Initialize the Optimizer\n",
    "            - params: the list of parameters\n",
    "            - lr: learning rate alpha\n",
    "            - betas: tuple of (beta_1, beta_2)\n",
    "            - eps: epsilon\n",
    "            - weight_decay: instance of class WeightDecay\n",
    "            - optimized_update: a flag whether to optimize the bias correction \n",
    "                                of the second moment by doing it after adding epsilon\n",
    "            - amsgrad: a flag indicating whether to use AMSGrad or fallback to plain Adam\n",
    "            - defaults: a dict of default for group values\n",
    "        '''\n",
    "        defaults = {} if defaults is None else defaults\n",
    "        defaults.update(dict(amsgrad=amsgrad))\n",
    "        \n",
    "        super().__init__(params, lr, betas, eps, weight_decay, optimized_update, defaults)\n",
    "        \n",
    "    def init_state(self, state: Dict[str, Any], group: Dict[str, Any], param: nn.Parameter):\n",
    "        '''\n",
    "        Initialize a parameter state\n",
    "            - state: the optimizer state of the parameter (tensor)\n",
    "            - group: stores optimizer attributes of the parameter group\n",
    "            - param: the parameter tensor theta at t-1\n",
    "        '''\n",
    "        # Enteding Adam opt\n",
    "        super().init_state(state, group, param)\n",
    "        \n",
    "        # if amsgrad = True, maintain the maximum of exponential moving average of squared gradient\n",
    "        if group['amsgrad']:\n",
    "            state['max_exp_avg_sqrd'] = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
    "    \n",
    "    def calc_mv(self, state: Dict[str, Any], group: Dict[str, Any], grad: Tensor):\n",
    "        '''\n",
    "        Calculate m_t and v_t or max(v1, v2, ..., vt)\n",
    "            - state: the optimizer state of the parameter (tensor)\n",
    "            - group: stores optimizer attributes of the parameter group\n",
    "            - grad: current gradient tensor g_t for theta at t-1\n",
    "        '''\n",
    "        m, v = super().calc_mv(state, group, grad)\n",
    "        \n",
    "        # if amsgrad, get max(v1, v2, ..., vt)\n",
    "        if group['amsgrad']:\n",
    "            v_max = state['max_exp_avg_sqrd']\n",
    "            torch.maximum(v_max, v, out=v_max)\n",
    "            return m, v_max\n",
    "        else:\n",
    "            return m, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 AMSGrad 논문에서 제시된 가상의 시나리오로, Adam이 실패하는 상황을 보여준다.\n",
    "\n",
    "$$\n",
    "f_t(x) = \n",
    "\\begin{cases} \n",
    "1010x, & \\text{for } t \\mod 101 = 1 \\\\\n",
    "-10x, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "~~where~-1\\leq x\\leq +1\n",
    "$$\n",
    "\n",
    "여기서 optimal solution은 $x=-1$이며, 옵티마이저의 performance는 다음(`regret`)을 이용해 측정한다.\n",
    "\n",
    "$$\n",
    "R(T) = \\sum^T_{t=1} [f_t(\\theta_t)-f_t(\\theta^*)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _adam_exp(is_adam: bool):\n",
    "    x = nn.Parameter(torch.tensor([.0]))\n",
    "    \n",
    "    # optimal: x^* = -1\n",
    "    x_prime = nn.Parameter(torch.tensor([-1]), requires_grad=False)\n",
    "    \n",
    "    # f_t(x)\n",
    "    def func(t: int, x_: nn.Parameter):\n",
    "        if t % 101 == 1:\n",
    "            return (1010 * x_).sum()\n",
    "        else:\n",
    "            return (-10 * x_).sum()\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    if is_adam:\n",
    "        optimizer = Adam([x], lr=1e-2, betas=(0.9,0.99))\n",
    "    else:\n",
    "        optimizer = AMSGrad([x], lr=1e-2, betas=(0.9,0.99))\n",
    "        \n",
    "    # R(T)\n",
    "    total_regret = 0\n",
    "    from labml import monit, tracker, experiment\n",
    "    \n",
    "    with experiment.record(name='synthetic', comment='Adam' if is_adam else 'AMSGrad'):\n",
    "        for step in monit.loop(10_000_000):\n",
    "            # f_t(theta_t) - f_t(theta^prime)\n",
    "            regret = func(step, x) - func(step, x_prime)\n",
    "            total_regret += regret.item()\n",
    "            \n",
    "            if (step+1) % 1000 == 0:\n",
    "                tracker.save(loss=regret, x=x, regret=total_regret / (step+1))\n",
    "            \n",
    "            # calculate gradients\n",
    "            regret.backward()\n",
    "            \n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # -1 <= x <= +1\n",
    "            x.data.clamp_(-1., +1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\"><span style=\"color: #C5C1B4\"></span>\n",
       "<span style=\"color: #C5C1B4\">--------------------------------------------------</span><span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\"></span></strong></span>\n",
       "<span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\">LABML WARNING</span></strong></span>\n",
       "<span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\"></span></strong></span>Not a valid git repository: <strong>/Users/kangbeenko/Desktop/GitHub_Repository/Pytorch_DeepLearning/02_Optimizers</strong><span style=\"color: #C5C1B4\"></span>\n",
       "<span style=\"color: #C5C1B4\">--------------------------------------------------</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Adam\n",
    "_adam_exp(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\"><span style=\"color: #C5C1B4\"></span>\n",
       "<span style=\"color: #C5C1B4\">--------------------------------------------------</span><span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\"></span></strong></span>\n",
       "<span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\">LABML WARNING</span></strong></span>\n",
       "<span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\"></span></strong></span>Not a valid git repository: <strong>/Users/kangbeenko/Desktop/GitHub_Repository/Pytorch_DeepLearning/02_Optimizers</strong><span style=\"color: #C5C1B4\"></span>\n",
       "<span style=\"color: #C5C1B4\">--------------------------------------------------</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AMSGrad\n",
    "_adam_exp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
