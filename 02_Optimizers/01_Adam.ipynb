{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: <a href='https://arxiv.org/abs/1412.6980'>Adam: A Method for Stochastic Optimization</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha, \\beta_1, \\beta_2$는 실수값 하이퍼파라미터, $m_t$와 $v_t$는 각각 1차 모멘텀과 2차 모멘텀일때, $\\hat{m}_t, \\hat{v}_t$는 업데이트가 진행된 이후 조정된 모멘텀의 값이다. \n",
    "$$\n",
    "m_t \\leftarrow \\beta_1 m_{t-1} + (1-\\beta_1)\\cdot g_t\n",
    "$$\n",
    "$$\n",
    "v_t \\leftarrow \\beta_2 v_{t-1} + (1-\\beta_2)\\cdot {g_t}^2\n",
    "$$\n",
    "$$\n",
    "\\hat{m}_t \\leftarrow {m_t\\over 1-{\\beta_1}^t}\n",
    "$$\n",
    "$$\n",
    "\\hat{v}_t \\leftarrow {v_t\\over 1-{\\beta_2}^t}\n",
    "$$\n",
    "\n",
    "최종 업데이트 식은 다음과 같다. 여기서 $\\epsilon$은 `division by zero` 문제 해결을 위해 분모에 더해주는 아주 작은 값으로, 하이퍼파라미터로써 조정할 수 있다.\n",
    "\n",
    "$$\n",
    "\\theta_t \\leftarrow \\theta_{t-1} - \\alpha \\cdot {\\hat{m}_t\\over \\sqrt{\\hat{v}_t}+\\epsilon} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\epsilon$이 0일 때 Effective step taken $\\Delta t$는 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\Delta t = \\alpha \\cdot {\\hat{m}_t\\over \\hat{v}_t}\n",
    "$$\n",
    "\n",
    "이는 $1-\\beta_1 > \\sqrt{1-\\beta_2}$이고 $|\\Delta t| \\leq \\alpha$일 때 다음과 같은 범위를 가진다.\n",
    "\n",
    "$$\n",
    "|\\Delta t| \\leq \\alpha \\cdot {1-\\beta_1 \\over \\sqrt{1-\\beta_2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from labml import tracker\n",
    "\n",
    "from optimizers import GenericAdaptiveOptimizer, WeightDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class Adam(GenericAdaptiveOptimizer):\n",
    "    def __init__(self, params, \n",
    "                 lr: float = 1e-3,\n",
    "                 betas: Tuple[float, float] = (0.9, 0.999),\n",
    "                 eps: float = 1e-16,\n",
    "                 weight_decay: WeightDecay = WeightDecay(),\n",
    "                 optimized_update: bool = True,\n",
    "                 defaults: Optional[Dict[str, Any]] = None):\n",
    "        '''\n",
    "        Initialize the Optimizer\n",
    "            - params: the list of parameters\n",
    "            - lr: learning rate alpha\n",
    "            - betas: tuple of (beta_1, beta_2)\n",
    "            - eps: epsilon\n",
    "            - weight_decay: instance of class WeightDecay\n",
    "            - optimized_update: a flag whether to optimize the bias correction \n",
    "                                of the second moment by doing it after adding epsilon\n",
    "            - defaults: a dict of default for group values\n",
    "        '''\n",
    "        defaults = {} if defaults is None else defaults\n",
    "        defaults.update(weight_decay.defaults())\n",
    "        super().__init__(params, defaults, lr, betas, eps)\n",
    "            \n",
    "        self.weight_decay = weight_decay\n",
    "        self.optimized_update = optimized_update\n",
    "    \n",
    "    def init_state(self, state: Dict[str, any],\n",
    "                   group: Dict[str, any],\n",
    "                   param: nn.Parameter):\n",
    "        '''\n",
    "        Initialize a parameter state\n",
    "            - state: the optimizer state of the parameter (tensor)\n",
    "            - group: stores optimizer attributes of the parameter group\n",
    "            - param: the parameter tensor theta at t-1\n",
    "        '''\n",
    "        state['step'] = 0 # the number of optimizer steps taken on t\n",
    "        # exponential moving avg of gradients, m_t\n",
    "        state['exp_avg'] = torch.zeros_like(param, memory_format=torch.preserve_format) \n",
    "        # exponeitial moving avg of squared gradient values, v_t\n",
    "        state['exp_avg_sqrd'] = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
    "    \n",
    "    def calc_mv(self, state: Dict[str, Any],\n",
    "                group: Dict[str, Any], grad: torch.Tensor):\n",
    "        '''\n",
    "        Calculate m_t and v_t\n",
    "            - state: the optimizer state of the parameter (tensor)\n",
    "            - group: stores optimizer attributes of the parameter group\n",
    "            - grad: current gradient tensor g_t for theta at t-1\n",
    "        '''\n",
    "        beta1, beta2 = group['betas']\n",
    "        m, v = state['exp_avg'], state['exp_avg_sqrd']\n",
    "        \n",
    "        # calculation of m_t (inplace calculation)\n",
    "        m.mul_(beta1).add_(grad, alpha=1-beta1)\n",
    "        # == beta1 * m + (1 - beta1) * grad\n",
    "        \n",
    "        # calculation of v_t\n",
    "        v.mul_(beta2).add_(grad**2, alpha=1-beta2)\n",
    "        \n",
    "        return m, v\n",
    "    \n",
    "    def get_lr(self, group: Dict[str, any]):\n",
    "        '''\n",
    "        returns the modified lr based on the state\n",
    "        '''\n",
    "        return group['lr']\n",
    "    \n",
    "    def update_adam(self, state: Dict[str, any],\n",
    "                    group: Dict[str, any],\n",
    "                    param: torch.nn.Parameter,\n",
    "                    m: torch.Tensor, v: torch.Tensor):\n",
    "        '''\n",
    "        Update the Adam parameter\n",
    "            - state: the optimizer state of the parameter (tensor)\n",
    "            - group: stores optimizer attributes of the parameter group\n",
    "            - param: the parameter tensor theta at t-1\n",
    "            - m, v: the uncorrected first and second moments m_t and v_t\n",
    "        '''\n",
    "        beta1, beta2 = group['betas']\n",
    "        \n",
    "        # bias correction term 1-beta1^t\n",
    "        bias_correction1 = 1 - beta1 ** state['step']\n",
    "        \n",
    "        # bias correction term 1-beta2^t\n",
    "        bias_correction2 = 1 - beta2 ** state['step']\n",
    "        \n",
    "        lr = self.get_lr(state, group)\n",
    "        \n",
    "        if self.optimized_update:\n",
    "            denominator = v.sqrt().add_(group['eps'])\n",
    "            step_size = lr * math.sqrt(bias_correction2) / bias_correction1\n",
    "        else:\n",
    "            # computation without optimization\n",
    "            denominator = (v.sqrt()/math.sqrt(bias_correction2)).add_(group['eps'])\n",
    "            step_size = lr / bias_correction1\n",
    "            \n",
    "        param.data.addcdiv_(m, denominator, value=-step_size)\n",
    "        \n",
    "    def step_param(self, state: Dict[str, Any], \n",
    "                   group: Dict[str, Any], grad: Tensor, param: Tensor):\n",
    "        '''\n",
    "        Take an update step for a given parameter tensor\n",
    "            - state: the optimizer state of the parameter (tensor)\n",
    "            - group: stores optimizer attributes of the parameter group\n",
    "            - grad: current gradient tensor g_t for theta at t-1\n",
    "            - param: the parameter tensor theta at t-1\n",
    "        '''\n",
    "        grad = self.weight_decay(param, grad, group)\n",
    "        m, v = self.calc_mv(state, group, grad)\n",
    "        \n",
    "        # increment t\n",
    "        state['step'] += 1\n",
    "        \n",
    "        # perform adam update\n",
    "        self.update_adam(state, group, param, m, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
