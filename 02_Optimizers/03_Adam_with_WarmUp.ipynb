{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam with Warm Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Adam with warmup`은 `Adam` 옵티마이저에 학습 초기에 lr을 서서히 올리는 '웜업(warmup)' 단계를 추가한 것이다. \n",
    "\n",
    "이 웜업 기간 동안 lr은 낮은 값에서 시작하여 점차 증가하며, 이는 모델이 더 안정적으로 학습을 시작하게 해 주어서 초기 학습의 불안정성을 줄이고 전체적인 학습 과정의 성능을 개선하는 데 도움을 준다. 특히, 큰 데이터셋이나 복잡한 모델에서 효과적이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Tuple, Optional\n",
    "\n",
    "from optimizers import WeightDecay, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamWarmup(Adam):\n",
    "    def __init__(self, params, betas: Tuple[float, float] = (0.9, 0.999), \n",
    "                 eps: float = 1e-16, weight_decay: WeightDecay = WeightDecay(), \n",
    "                 optimized_update: bool = True, warmup: int = 0, \n",
    "                 min_lr: float = 1e-8, max_lr: float = 1e-3,\n",
    "                 defaults: Dict[str, Any] | None = None):\n",
    "\n",
    "        '''\n",
    "        Initialize the Optimizer\n",
    "            - params: the list of parameters\n",
    "            - betas: tuple of (beta_1, beta_2)\n",
    "            - lr: (maximum) learning rate alpha\n",
    "            - eps: epsilon\n",
    "            - weight_decay: instance of class WeightDecay\n",
    "            - optimized_update: a flag whether to optimize the bias correction \n",
    "                                of the second moment by doing it after adding epsilon\n",
    "            - warmup: number of warmup steps\n",
    "            - min_lr: minimum learning rate to be the starting point for warmup phase\n",
    "            - defaults: a dict of default for group values\n",
    "        '''\n",
    "        defaults = {} if defaults is None else defaults\n",
    "        defaults.update({\n",
    "            warmup : warmup,\n",
    "            min_lr : min_lr,\n",
    "            })\n",
    "        \n",
    "        super().__init__(params, betas, lr, eps, weight_decay, optimized_update, defaults)\n",
    "    \n",
    "    def get_lr(self, state: Dict[str, any], group: Dict[str, any]):\n",
    "        # if current step is in warmup stage\n",
    "        if group['warmup'] > state['step']:\n",
    "            # linearly increasing lr from 0 to alpha\n",
    "            return min_lr + state['step'] * group['lr'] / group['warmup']\n",
    "        else:\n",
    "            return group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
