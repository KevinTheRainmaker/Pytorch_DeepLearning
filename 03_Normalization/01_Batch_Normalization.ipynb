{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: <a href='https://arxiv.org/abs/1502.03167'>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 정규화(Batch Normalization)는 신경망의 각 층에서 입력 데이터의 분포를 안정화시키는 방법이다.\n",
    "\n",
    "일반적으로 신경망에서는 학습 과정 중에 가중치와 편향이 업데이트되면서 입력 데이터의 분포가 변화할 수 있는데, 이를 논문에서는 `Internal Covariate Shift`라고 정의한다.\n",
    "\n",
    "데이터의 분포가 레이어를 거치면서 변화할 경우 후속 레이어가 이와 같이 shifted 된 분포에 적응을 해야하는 문제가 발생하기 때문에 학습 속도가 느려질 수 있으며, 가중치 초기값에 따라 학습 결과가 크게 달라질 수 있다는 문제점을 가지게 된다. 이러한 문제는 층이 깊어질수록 더욱 심각해질 수 있기에 딥러닝 연구에서 매우 중요한 문제 중 하나이다.\n",
    "\n",
    "배치 정규화는 이러한 문제를 해결하기 위해 각 층의 활성화 함수(activation function)의 입력을 평균과 분산을 사용하여 정규화함으로써 입력의 분포를 안정화시키고 학습속도를 향상시키며, 가중치 초기화에 덜 민감하게 만든다.\n",
    "\n",
    "메인 아이디어는 각 층의 활성화 값이 정규화되어 특정 범위에 머무르도록 하는 것이며 다음과 같은 단계를 거친다.\n",
    "\n",
    "1. 배치 평균 및 분산 계산: 각 미니 배치 별로 입력 $x$의 평균 $E[x]$와 분산 $Var[x]$를 계산하고 입력을 정규화한다.\n",
    "$$ \\hat{x}={x-\\mathbb{E}[x]\\over \\sqrt{Var[x]+\\epsilon}}$$ \n",
    "\n",
    "2. 스케일 및 시프트: 정규화된 입력 $\\hat{x}$에 학습 가능한 스케일 파라미터 $\\gamma$와 시프트 파라미터 $\\beta$를 적용하여 네트워크가 적절한 분포를 학습할 수 있도록 한다.\n",
    "\n",
    "$$ \\hat{x}^{(k)}={x^{(k)}-\\mathcal{E}[x^{(k)}]\\over \\sqrt{Var[x^{(k)}]}}$$\n",
    "\n",
    "$$y^{(k)} = \\gamma^{(k)}\\hat{x}^{(k)}+\\beta^{(k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: 외부에서 미리 계산된(outside gradient computed) 평균을 사용할 경우 $x$의 변화에 따른 $E[x]$의 변화가 반영되지 않게 되어 신경망에서 편향 등의 파라미터가 적절히 조정되지 않을 수 있다. 따라서 배치 정규화에서는 미니 배치 내에서 계산된 평균을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추론 시 활용되는 데이터셋에 대해 평균과 분산을 계산할 수도 있고, 학습 과정 중에 계산된 값을 활용할 수도 있다. 일반적인 경향은 훈련 단계 중 이동지수평균(exponential moving average)을 이용하여 평균과 분산을 계산 후 추론 시 활용하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X\\in \\mathbb{R}^{B\\times C\\times H\\times W}$를 입력으로 받는 Batch Normalization Layer는 X를 다음과 같이 정규화한다. (B는 배치사이즈, C는 채널의 수, H는 height, W는 width)\n",
    "\n",
    "$$\n",
    "BN(X) = \\gamma {X-\\mathbb{E}_{B,H,W}[X] \\over \\sqrt{{Var}_{B,H,W}[X] + \\epsilon}} + \\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* C는 $\\gamma$와 $\\beta$의 차원이 되며 나머지 값(B,H,W)에 대해서 평균과 분산이 계산됨\n",
    "\n",
    "$$\n",
    "if~X\\in \\mathbb{R}^{B\\times C\\times L},~\\gamma\\in\\mathbb{R}^C,~\\beta\\in\\mathbb{\\R}^C~and~\\mathbb{E}_{B,L}[X],~{Var}_{B,L}[X]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    '''\n",
    "    channels: 입력의 피쳐 수\n",
    "    eps: 분모에 사용되는 엡실론. div-by-zero 에러 방지\n",
    "    momentum: 이동지수평균을 위한 모멘텀\n",
    "    affine: 정규화된 값에 대해 스케일 및 시프트 적용 여부 결정\n",
    "    track_running_stats: 평군과 분산에 대한 이동평균 계산 여부 결정\n",
    "    '''\n",
    "    def __init__(self, channels: int, *,\n",
    "                 eps: float = 1e-5, momentum: float = 0.1,\n",
    "                 affine: bool = True, track_running_stats: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.channels = channels\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "        \n",
    "        # parameters for scale and shift\n",
    "        if self.affine:\n",
    "            self.gamma = nn.Parameter(torch.ones(channels)) # 곱하기 1\n",
    "            self.beta = nn.Parameter(torch.zeros(channels)) # 더하기 0\n",
    "        \n",
    "        # parameters for exponential moving average\n",
    "        if self.track_running_stats:\n",
    "            self.register_buffer('exp_mean', torch.zeros(channels)) # 평균 0\n",
    "            self.register_buffer('exp_var', torch.ones(channels)) # 분산 1\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # shape(x) = (batch_size, channels, dimensions*)\n",
    "        # ex - 2D image x will be (batch_size, channels, height, width)\n",
    "        \n",
    "        x_shape = x.shape\n",
    "        batch_size = x_shape[0]\n",
    "        \n",
    "        assert self.channels == x_shape[1]\n",
    "        \n",
    "        # reshape into [batch_size, channels, n]\n",
    "        x = x.view(batch_size, self.channels, -1)\n",
    "        \n",
    "        # calculate batch mean & var in training mode,\n",
    "        # or if we have not tracked exponential moving average\n",
    "        if self.training or not self.track_running_stats:\n",
    "            mean = x.mean(dim=[0,2]) # first and last dimension\n",
    "            mean_x2 = (x ** 2).mean(dim=[0,2])\n",
    "            \n",
    "            var = mean_x2 - (mean ** 2)\n",
    "        \n",
    "        # update exponential moving averages\n",
    "        if self.training and self.track_running_stats:\n",
    "            self.exp_mean = (1-self.momentum) * self.exp_mean + self.momentum * mean\n",
    "            self.exp_var = (1-self.momentum) * self.exp_var + self.momentum * var\n",
    "        else:\n",
    "            # use exponential moving averages as estimates\n",
    "            mean = self.exp_mean\n",
    "            var = self.exp_var\n",
    "            \n",
    "        # normalize\n",
    "        x_norm = (x - mean.view(1,-1,1)) / torch.sqrt(var + self.eps).view(1,-1,1)\n",
    "        \n",
    "        # scale and shift\n",
    "        if self.affine:\n",
    "            x_norm = self.gamma.view(1,-1,1) * x_norm + self.beta.view(1,-1,1)\n",
    "        \n",
    "        return x_norm.view(x_shape) # reshape to original shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test():\n",
    "    x = torch.randn([2,3,2,4]) # B C H W\n",
    "    print(f'original input: {x})')\n",
    "    \n",
    "    \n",
    "    bn = BatchNorm(3) # channel = 3\n",
    "    x = bn(x)\n",
    "    print(f'normalized input: {x}')\n",
    "    print(f'{bn.exp_mean = }')\n",
    "    print(f'{bn.exp_var = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original input: tensor([[[[ 0.5892,  0.1462,  1.3988,  0.1927],\n",
      "          [-1.0795, -1.0181, -1.0973,  1.6883]],\n",
      "\n",
      "         [[-0.1495,  0.8476,  0.6319, -0.1521],\n",
      "          [ 0.8162,  0.3071, -1.1146,  0.2999]],\n",
      "\n",
      "         [[-0.1654, -0.9820,  0.4337, -0.0719],\n",
      "          [ 0.2602, -0.2326, -1.0214, -0.2457]]],\n",
      "\n",
      "\n",
      "        [[[-2.2961, -2.1283,  1.1741, -0.2161],\n",
      "          [ 0.2236, -0.4049, -0.7777, -0.6352]],\n",
      "\n",
      "         [[-1.5205,  0.0137,  0.3692, -1.3053],\n",
      "          [ 0.5722,  0.4965,  0.1500, -0.8167]],\n",
      "\n",
      "         [[-0.4041,  0.8192, -0.6540,  1.6048],\n",
      "          [-0.2825,  1.2179, -2.2571, -0.2425]]]]))\n",
      "normalized input: tensor([[[[ 0.7651,  0.3683,  1.4903,  0.4100],\n",
      "          [-0.7295, -0.6745, -0.7454,  1.7495]],\n",
      "\n",
      "         [[-0.1561,  1.1998,  0.9064, -0.1597],\n",
      "          [ 1.1570,  0.4647, -1.4686,  0.4549]],\n",
      "\n",
      "         [[-0.0297, -0.9479,  0.6439,  0.0754],\n",
      "          [ 0.4489, -0.1052, -0.9922, -0.1200]]],\n",
      "\n",
      "\n",
      "        [[[-1.8192, -1.6689,  1.2890,  0.0438],\n",
      "          [ 0.4377, -0.1253, -0.4592, -0.3315]],\n",
      "\n",
      "         [[-2.0205,  0.0657,  0.5492, -1.7278],\n",
      "          [ 0.8253,  0.7223,  0.2512, -1.0635]],\n",
      "\n",
      "         [[-0.2981,  1.0773, -0.5791,  1.9606],\n",
      "          [-0.1614,  1.5256, -2.3817, -0.1164]]]], grad_fn=<ViewBackward0>)\n",
      "bn.exp_mean = tensor([-0.0265, -0.0035, -0.0139])\n",
      "bn.exp_var = tensor([1.0247, 0.9541, 0.9791])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    _test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
