{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: <a href='https://arxiv.org/abs/1607.06450'>Layer Normalization</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization의 한계\n",
    "배치 정규화는 미니 배치에 대해서 summed input의 평균과 분산을 계산하여 정규화를 수행한다. 이러한 방식은 내부 공변량 변화(Internal Covariate Shift)를 해결하여 일반화 개선 및 오버피팅 감소, 학습 안정성 향상 및 기울기 소실 문제 완화, 그리고 빠른 학습속도 및 가중치 초기화에 대해 강건해지는 장점을 가지지만 일부 단점이 존재한다.\n",
    "\n",
    "1. 미니 배치 크기에 의존하며, 작은 크기의 미니 배치(ex - `batch_size = 1`)에 대해서는 원활히 동작하지 않게 된다. 최근의 LLM을 포함한 Large NLP Model들은 작은 크기의 배치 사이즈를 가지므로 이는 중요한 문제이다.\n",
    "2. 각 시점(time step)마다 서로 다른 데이터가 연속적으로 입력되는 Sequential 형태의 데이터를 다루는 RNN 등 Recurrent 모델에서는 배치 정규화를 적용하기 어렵다.\n",
    "3. 분산 학습 환경에서 기기간 평균 및 분산을 모두 계산하여야 하기 때문에 계산 효율성이 저하된다.\n",
    "4. test 단계에서 inference 시 train 과정에서의 평균 및 분산을 계속 저장하여 대신 사용해주어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "Layer Normalization은 BN의 한계점을 극복하기 위해 고안된 정규화 기법으로, 입력 데이터 전체 피처에 대해 평균 0, 분산 1을 갖도록 변환시킨다. 이는 BN에서 각 element마다 정규화되었던 것과 차이가 존재한다.\n",
    "\n",
    "LN은 배치가 아닌 레이어를 기준으로 정규화를 수행함으로써 BN이 가지고 있던 배치 크기에 대한 의존도를 제거하였다. 또한 sequence에 따른 고정 길이 정규화가 이루어지기 때문에 Recurrent 기반 모델에도 적용이 수월하다. \n",
    "LN은 주로 NLP task에 많이 사용되며 트랜스포머 계열의 구현에서도 자주 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 $X\\in \\mathbb{R}^{L\\times B\\times C}$에 대하여 $LN(X) = \\gamma {X - \\mathbb{E}_{C}[X] \\over \\sqrt{{Var}_{C}[X] + \\epsilon}} + \\beta$가 된다. (L, B의 값과 무관하게 계산)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "\n",
    "import torch\n",
    "from torch import nn, Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    '''\n",
    "    normalized_shape: element의 shape S에 대하여, 입력 X의 shape는 [*, S[0], S[1], ..., S[n]] (*는 차원 수 / sequence에서는 seq_len이 될 수 있음)\n",
    "    eps: 엡실론\n",
    "    elementwise_affine: 정규화된 값에 대해 스케일 및 시프트 적용 여부 결정\n",
    "    '''\n",
    "    def __init__(self, normalized_shape: Union[int, List[int], Size],\n",
    "                 eps: float = 1e-5, elementwise_affine: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convert `normalized_shape` to `torch.Size`\n",
    "        if isinstance(normalized_shape, int):\n",
    "            normalized_shape = torch.Size([normalized_shape])\n",
    "        elif isinstance(normalized_shape, list):\n",
    "            normalized_shape = torch.Size(normalized_shape)\n",
    "        assert isinstance(normalized_shape, torch.Size)\n",
    "        \n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "        \n",
    "        # gamma and beta for affine\n",
    "        if self.elementwise_affine:\n",
    "            self.gamma = nn.Parameter(torch.ones(normalized_shape))\n",
    "            self.beta = nn.Parameter(torch.zeros(normalized_shape))\n",
    "            \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        assert self.normalized_shape == x.shape[-len(self.normalized_shape):]\n",
    "        \n",
    "        # dimensions to calculate mean and variance\n",
    "        dims = [-(i + 1) for i in range(len(self.normalized_shape))]\n",
    "        \n",
    "        mean = x.mean(dim=dims, keepdim=True)\n",
    "        mean_x2 = (x ** 2).mean(dim=dims, keepdim=True)\n",
    "        \n",
    "        var = mean_x2 - (mean ** 2)\n",
    "        \n",
    "        # layer normalize\n",
    "        x_norm = (x-mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        # scale and shift\n",
    "        if self.elementwise_affine:\n",
    "            x_norm = self.gamma * x_norm + self.beta\n",
    "        \n",
    "        return x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test():\n",
    "    x = torch.randn([2,3,2,4]) # B C H W\n",
    "    print(f'original input: {x})')\n",
    "    \n",
    "    \n",
    "    ln = LayerNorm(x.shape[2:])\n",
    "    x = ln(x)\n",
    "    \n",
    "    print(f'normalized input: {x}')\n",
    "    print(f'{ln.gamma.shape = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original input: tensor([[[[-1.0389, -0.5300, -0.2023,  0.7930],\n",
      "          [ 1.1393,  0.2385,  0.8208, -2.2994]],\n",
      "\n",
      "         [[-0.4791, -0.3841,  1.8926,  1.7519],\n",
      "          [ 0.3365, -0.9453, -0.5782,  0.5030]],\n",
      "\n",
      "         [[-0.1186, -0.1813, -0.4453,  0.3676],\n",
      "          [ 0.8719, -1.2697,  0.1110, -0.0684]]],\n",
      "\n",
      "\n",
      "        [[[-0.7527,  0.8848,  1.3261, -1.1935],\n",
      "          [-1.3128, -2.1940,  0.6254,  0.3473]],\n",
      "\n",
      "         [[-0.1483,  2.2627, -0.3401, -0.4508],\n",
      "          [ 0.1664, -0.7996, -0.2658, -0.3954]],\n",
      "\n",
      "         [[-0.2540, -0.6572, -0.4892, -1.1827],\n",
      "          [ 1.4013,  0.0369,  0.3618,  0.8968]]]]))\n",
      "normalized input: tensor([[[[-0.8430, -0.3684, -0.0629,  0.8653],\n",
      "          [ 1.1881,  0.3482,  0.8911, -2.0184]],\n",
      "\n",
      "         [[-0.7380, -0.6433,  1.6231,  1.4830],\n",
      "          [ 0.0740, -1.2020, -0.8365,  0.2398]],\n",
      "\n",
      "         [[-0.0465, -0.1543, -0.6085,  0.7901],\n",
      "          [ 1.6577, -2.0269,  0.3485,  0.0399]]],\n",
      "\n",
      "\n",
      "        [[[-0.4012,  0.9993,  1.3768, -0.7781],\n",
      "          [-0.8801, -1.6338,  0.7775,  0.5396]],\n",
      "\n",
      "         [[-0.1704,  2.5347, -0.3857, -0.5098],\n",
      "          [ 0.1826, -0.9012, -0.3023, -0.4478]],\n",
      "\n",
      "         [[-0.3381, -0.8464, -0.6347, -1.5090],\n",
      "          [ 1.7488,  0.0286,  0.4382,  1.1127]]]], grad_fn=<AddBackward0>)\n",
      "ln.gamma.shape = torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    _test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
